#+title: Synthesis of Artefact coding
#+property: header-args :db "data.db" :header yes :noweb yes

#+begin_src sqlite
  .schema data
#+end_src

#+RESULTS:
| CREATE TABLE data (                    |   |
| Acronym VARCHAR NOT NULL               |   |
| Title VARCHAR NOT NULL                 |   |
| Ref VARCHAR                            |   |
| Date VARCHAR                           |   |
| Organisation VARCHAR NOT NULL          |   |
| Affiliation VARCHAR                    |   |
| URL VARCHAR                            |   |
| Artifacts collected VARCHAR            |   |
| Mandatory or Optional VARCHAR          |   |
| Submission Requirements VARCHAR        |   |
| Artifacts license VARCHAR              |   |
| Evaluation criteria VARCHAR            |   |
| Evaluation Process VARCHAR             |   |
| Documentaion VARCHAR                   |   |
| Artifact claims VARCHAR                |   |
| Artifact Available VARCHAR             |   |
| Virtual Environments VARCHAR           |   |
| Artifact Functional review VARCHAR     |   |
| Artifact Reusable VARCHAR              |   |
| Results Reproduced VARCHAR             |   |
| Results Replicated VARCHAR             |   |
| Specialized hardware VARCHAR           |   |
| Confidentiality VARCHAR                |   |
| Proprietary Software VARCHAR           |   |
| Long-Running Computations VARCHAR      |   |
| Unstable or Dangerous Software VARCHAR |   |
| Open question VARCHAR                  |   |
| Metadata VARCHAR                       |   |
| Badge Validator VARCHAR                |   |
| Badge Revocation VARCHAR               |   |
| );                                     |   |

* What conferences did we look at?

#+begin_src sqlite 
  select distinct acronym, title from data;
#+end_src

#+RESULTS:
| Acronym          | Title                                   |
| ACM              | Artifact Review and Badging Version 1.1 |
| CHES 2025        | Artifact Evaluation                     |
| CHI 2024         | Artifacts at CHI 2024                   |
| CGO 2025         | Artifact Evaluation                     |
| DSN 2025         | Artifacts Call For Contributions        |
| ICFP 2025        | ICFP Artifacts                          |
| IEEE             | Reproducibility Badges                  |
| NDSS 2025        | NDSS Symposium 2025 Call for Artifacts  |
| PLDI 2025        | PLDI Research Artifacts                 |
| POPL 2025        | Artifact Evaluation                     |
| USENIX Sec 2025  | Artifact Evaluation                     |
| NISO             | "Reproducibility Badging                |
| and Definitions" |                                         |
| OSDI 2025        | OSDI '25 Call for Artifacts             |


#+name: query
#+begin_src sqlite :noweb yes :var field=""
  select distinct Acronym, field from data where field <> "";
#+end_src

#+RESULTS: query

* TODO Artifacts collected

Coding needs rereview.


#+begin_src sqlite :results raw :wrap verbatim :line yes
  select distinct Acronym, "Artifacts Collected" from data where "Artifacts Collected" <> "";
#+end_src

#+RESULTS:
#+begin_verbatim
            Acronym = ACM
Artifacts collected = By "artifact" we mean a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.

            Acronym = CHES 2025
Artifacts collected = Software implementations (performance, formal verification, etc.): The source code of the implementation; a list of all dependencies required; the test harness; instructions on how to build and run the software and the test harness; a description of the platform on which the results in the paper were obtained; and instructions or scripts to process the output of the test harness into appropriate summary statistics.
Hardware implementations, physical attacks against implementations: A precise description of any physical equipment used in the setup; the source code of any software developed for the experiment; a list of all dependencies required; instructions on how to build the software and run the device or carry out the attack; instructions or scripts to process the output and interpret the results.
Data or other non-code artifacts: Documents or reports in a widely used non-proprietary format, such as PDF, ODF, HTML, text; data in machine-readable format such as CSV, JSON, XML, with appropriate metadata describing the schema; scripts used to process the data into summary form. Where non-standard data formats cannot be avoided, authors should include suitable viewing software.

            Acronym = CHI 2024
Artifacts collected = Video
You can upload additional video artifacts, which are different to the
video presentation and video preview you can already provide.  This
could be a video demonstration of a hardware prototype, a video
walkthrough of a virtual world, or any other videos that underpin your
publication.  Audio You can upload audio files as artifacts that
support your publication.  This could be recordings of sounds critical
to your paper, audio recordings of oral data, or any other audio that
underpins your publication.  Software You can upload the software that
supports your publication.  This could be experimental software,
software that runs simulations, code that reproduces analysis, or any
other software artifact that underpins your publication.  Datasets You
can upload datasets that support your publication.  This could be logs
from empirical work, data used in your analysis, transcripts, or any
other data that underpins your publication.  Presentation Slides You
can upload presentation slides that support your publication.  This
could be the presentation which you will give at the conference, or
any presentation that underpins your publication.  Other There are
many artifacts not covered by the current categories supported by ACM,
but that are still valuable to include with your publication.  This
could be survey and interview protocols, details of experiment
pre-registration, technical diagrams like CAD files, 3D models, or any
other artifact that underpins your publication.

            Acronym = CGO 2025
Artifacts collected = By “artifact” we mean a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.

            Acronym = DSN 2025
Artifacts collected = Artifacts can be Code or Datasets. The same research paper can be accompanied by both Code and Datasets.

            Acronym = ICFP 2025
Artifacts collected = An artifact that supports the paper’s conclusions can take many forms, including:
- a working copy of the software and its dependencies, including benchmarks, examples and/or case studies
- experimental data sets
- a mechanized proof

            Acronym = IEEE
Artifacts collected = Some articles in IEEE Xplore have Code and/or Datasets that have been submitted by authors along with published works.

            Acronym = PLDI 2025
Artifacts collected = The artifact evaluation will accept any artifact that authors wish to submit, broadly defined. A submitted artifact might be:
software
mechanized proofs
test suites
data sets
hardware (if absolutely necessary)
a video of a difficult- or impossible-to-share system in use any other artifact described in a paper

            Acronym = POPL 2025
Artifacts collected = Paper artifacts are the software, mechanized proofs, test suites, and benchmarks that support a research paper and evaluate its claims
Artifacts can be software, mechanical proofs, test suites, benchmarks, or anything else that bolsters the claims of the paper, except paper proofs, which the AEC lacks the time and expertise to carefully review


            Acronym = USENIX Sec 2025
Artifacts collected = Artifacts can include models, data files, proprietary binaries, exploits under embargo, etc.
#+end_verbatim

| Acronym         | Collected                                |
|-----------------+------------------------------------------|
| ACM             | data,software,source                     |
| CHES 2025       | tests,hardware,data,source               |
| CHI 2024        | demo,data,software,presentation,protocol |
| CGO 2025        | software,data                            |
| DSN 2025        | software,data                            |
| ICFP 2025       | software,tests,data                      |
| IEEE            | software,data                            |
| PLDI 2025       | software,tests,data,demo,hardware        |
| POPL 2025       | software,tests,data                      |
| USENIX SEC 2025 | data,software                            |

** Codebook

- source :: Readable source code and scripts as opposed to black-box analysis tools, so that you can see how it did it.
- hardware :: Full schematics and build instructions of any custom hardware, potentially hardware for archive as well
- software :: Any software (even proprietary software), mechanized proofs, and dependencies required to demonstrate your results, show the tool you made, with the goal of replicating your results, the software environment to run the code.  There is no expectation that the code ought to be inspectible, just that something ought to run.
- data :: Input and output datasets from your code to prove your results and support your conclusions.
- documentation :: What the heck is this?  What do I do with it?
- demo :: Typically video recordings demonstrating hard to replicate results.
- presentation :: Your slides
- protocol :: Documentation of your study methods and how you interacted with people.
- tests :: Evidence that your code is correct or that it meets performance guarantees
  
** Second coding
- Removed VM merged under code
- Added "source" as a code for readable source code as opposed to black-box analysis tools and tweaked definitions.
- Added "documentation" as a code for instructions on how to run the software.

* Submission Requirements
#+begin_src sqlite :results raw :wrap verbatim :line yes
  select distinct Acronym, "Artifacts license" from data where "Artifacts license" <> ""
#+end_src

#+RESULTS:
#+begin_verbatim
          Acronym = DSN 2025
Artifacts license = Artifacts should be submitted with a license that allows researchers to reuse and to extend the artifact (e.g., for comparison purposes in a future paper). The license can be indicated through metadata on the open-data repository, and through a file included in the artifact (e.g., LICENSE.txt). Creative Commons licenses are a typical choice for open data.

          Acronym = NDSS 2025
Artifacts license = Furthermore, for this badge, authors should provide a README file referencing the paper and a LICENSE file for the materials

          Acronym = PLDI 2025
Artifacts license = Some benchmark code is subject to licensing or intellectual property restrictions and cannot legally be shared with reviewers (e.g., licensed benchmark suites like SPEC, or when a tool is applied to private proprietary code). In such cases, all available benchmarks should be included. If all benchmark data from the paper falls into this case, alternative data should be supplied: providing a tool with no meaningful inputs to evaluate on is not sufficient to justify claims that the artifact works.

          Acronym = POPL 2025
Artifacts license = Reusability Guidelines
Reusable artifacts should be released under an open-source license (e.g., OSI list). Additionally, see the following additional instructions for specific artifact types.

          Acronym = NISO
Artifacts license = 1. This is akin to author-supplied supplemental materials, shared under a standard public license such as an Open Science Initiative (OSI)–approved license for software or a Creative Commons license or public domain dedication for data and other materials.
#+end_verbatim

| Acronym   | Codes                   |
|-----------+-------------------------|
| DSN 2025  | reuse,CC,license |
| NDSS 2025 | license                 |
| PLDI 2025 |                         |
| POPL 2025 | reuse,CC                |
| NISO      | CC                      |

** Codebook
- reuse :: you should be able reuse and extend any artefact you submit
- CC :: It should be under a Creative Commons or OSI license or in the public domain
- license :: You should explicitly state what license it is under (via a README file)


* Evaluation Criteria

#+begin_src sqlite :results raw :wrap verbatim :line yes
  select distinct Acronym, "Evaluation Criteria" from data where "Evaluation Criteria" <> ""
#+end_src

#+RESULTS:
#+begin_verbatim
            Acronym = ACM
Evaluation criteria = This badge is applied to papers whose associated artifacts have successfully completed an independent audit. Artifacts need not be made publicly available to be considered for this badge. However, they do need to be made available to reviewer.

            Acronym = ACM
Evaluation criteria = Documented: At minimum, an inventory of artifacts is included, and sufficient description provided to enable the artifacts to be exercised.
Consistent: The artifacts are relevant to the associated paper, and contribute in some inherent way to the generation of its main results.
Complete: To the extent possible, all components relevant to the paper in question are included. (Proprietary artifacts need not be included. If they are required to exercise the package then this should be documented, along with instructions on how to obtain them. Proxies for proprietary data should be included so as to demonstrate the analysis.)
Exercisable: Included scripts and/or software used to generate the results in the associated paper can be successfully executed, and included data can be accessed and appropriately manipulated.

            Acronym = ACM
Evaluation criteria = Artifacts do not need to have been formally evaluated in order for an article to receive this badge. In addition, they need not be complete in the sense described above. They simply need to be relevant to the study and add value beyond the text in the article. Such artifacts could be something as simple as the data from which the figures are drawn, or as complex as a complete software system under study.

            Acronym = CHI 2024
Evaluation criteria = Any author who provides artifacts using the form in PCS will be awarded an Artifact Available badge for their CHI 2024 paper.  The artifact Available badge asserts that author created artifacts relevant to the publication are available in an archival repository. These badges are not externally reviewed, and do not need to be comprehensive how they underpin the associated publication. 

            Acronym = ICFP 2025
Evaluation criteria = To facilitate reproduction and reuse, an artifact should be:
consistent with the claims and results presented in the paper;
as complete as possible, supporting all claims of the paper;
well-documented;
future-proof;
easy to extend and modify.

            Acronym = ICFP 2025
Evaluation criteria = Instructions for Common Types of Artifacts
Command-line Tools
Unix command-line tools should have standard --help-style command-line help pages. It is not acceptable for an executable to throw uninformative exceptions when executed with no flags or with the wrong flags.

Compilers and Interpreters
It should be obvious how to run the tool on new examples that the reviewers write themselves. Do not just hard-code the examples described in the paper.

If your tool consumes expressions in a custom DSL, then we recommend supplying a grammar for the concrete syntax, so that reviewers can try the tool on new examples. Papers that describe such languages often give just an abstract syntax, and it is often not clear what the full concrete syntax is from the paper alone.

Proof Scripts
In most cases, the artifact VM should contain an installation of the proof checker and specify a single command (preferably make) to re-check the proof. It is fine to leave the VM itself command-line only and require reviewers to browse the proof script locally on their own machines. It should not be necessary to have an IDE (e.g. CoqIDE, Emacs, or VSCode) installed into the VM, unless the paper is specifically about IDE functionality.

Include comments in the proof scripts that highlight the main theorems described in the paper. Use comments like “This is Theorem 1.2: Soundness described on page 6 of the paper”. Proof scripts written in “apply style” are typically unreadable without loading them into an IDE, but reviewers will still want to find the main lemmas and understand how they relate.

Reviewers often complain about a lack of comments in proof scripts. To authors, the logical statements of the lemmas themselves may be quite readable, but reviewers typically want English prose that repeats the same information.

Your proof artifact should also provide a command that gathers and prints the axioms that your proof relies on. For example, with the Coq/Rocq proof assistant, the Makefile produced by coq_makefile contains the target validate that does this. An instruction in your Readme.md file such as “look in the development for the admit keyword” is not acceptable.

            Acronym = ICFP 2025
Evaluation criteria = Web Interfaces
If your artifact has a web interface, try to get the server running locally inside the VM and allow the reviewer to connect to it via a web browser running natively on their host machine. Graphical environments installed into VMs are sometimes laggy and unstable, and standard web protocols are stable enough that such artifacts should be usable with any recent browser.

Programs that Generate Images
If the artifact produces an image file (e.g., a graph), then expect the reviewers to use scp or some such to copy it out to the host machine and view it. Authors should test that the connection to the VM works, as explained in the VM image’s README.md, so that this is possible.

            Acronym = PLDI 2025
Evaluation criteria = Consistency: the artifact should be relevant to the paper and can in principle reproduce the main results reported in the paper.
Completeness: the artifact can in principle reproduce all the results that the paper reports, and should include everything (code, tools, 3rd party libraries, etc.) required to do so.
Documentation: the artifact should be well documented so that generating the results is easy and transparent.
Ease of reuse: the artifact provides everything needed to build on top of the original work, including source files together with a working build process that can recreate the binaries provided.

            Acronym = POPL 2025
Evaluation criteria = Artifacts are evaluated against the criteria of:
Consistency with the claims of the paper and the results it presents
Completeness insofar as possible, supporting all evaluated claims of the paper
Documentation to allow easy reproduction by other researchers
Reusability, facilitating further research

            Acronym = USENIX Sec 2025
Evaluation criteria = Artifact Evaluation Committee will then evaluate for availability. Optionally, these artifacts can also be assessed for functionality and reproducibility.

            Acronym = NISO
Evaluation criteria = 2.2 Research Objects Reviewed (ROR)
This badge signals that all relevant author-created digital objects used in the research (including data and code) were reviewed according to the criteria provided by the badge issuer. The badge metadata should link to the award criteria.
Notes:
1. A publication may be awarded the ROR badge while not being eligible for the ORO badge, and vice-versa.
2. The criteria for review of the research objects (e.g., code) need to be determined by the editorial boards, community leaders, and other stakeholders. Some efforts are underway to develop standard criteria for code review. An example set of criteria is that of the Journal of Open Source Software (https://joss.readthedocs.io/en/latest/review_criteria.html).
3. This badge corresponds to the ACM “Artifacts Evaluated” badge, while the Institute of Electrical and Electronics Engineers (IEEE) has used a “Code Reviewed” badge (see Appendix A).
4. Badge issuers may signal different levels of review through qualifiers identified with the badge. Examples include the Functional and Reusable levels in ACM’s “Artifacts Evaluated” badge, or criteria associated with reproducibility.
#+end_verbatim

| Acronym         | Codes                                   |
|-----------------+-----------------------------------------|
| ACM             | audited,documented,consistent,runable   |
| CHI 2024        |                                         |
| ICFP 2025       | consistent,documented,extensible,usable |
| PLDI 2025       | consistent,documented,extensible        |
| POPL 2025       | consistent,documented,extensible        |
| USENIX Sec 2025 | audited,consistent,runable              |
| NISO            | audited                                 |


** Codebook

- audited :: Your artifact will be explicitly checked against your claims.
- documented :: There should be a guide to what your artifact is and how to run it. Code ought to be commented.
- consistent :: Your artifact should be tied to the results claimed in your paper.
- runable :: You should be able to "run" your artifact (whatever that means for you're artifact).
- extensible :: You should be able to extend and reuse your artifact
- usable :: Some consideration should have been made to the people trying to use your artifact.  Standard conventions ought to be followed for UIs and standard utility tools ought to be provided to let a reviewer get data out of an artifact's VM.
  

* Evaluation Process

#+begin_src sqlite :results raw :wrap verbatim :line yes
  select distinct Acronym, "Evaluation Process" from data where "Evaluation Process" <> "";
#+end_src

#+RESULTS:
#+begin_verbatim
           Acronym = ACM
Evaluation Process = The descriptions of badges provided above do not specify the details of the review process itself. For example: Should reviews occur before or after acceptance of a paper? How many reviewers should there be? Should the reviewers be anonymous, or should they be allowed to interact openly with the authors? How should artifacts be packaged for review? What specific metrics should be used to assess quality? Current grassroots efforts to evaluate artifacts and formally test replicability have answered these questions in different ways. We believe that it is still too early to establish more specific guidelines for artifact and replicability review. Indeed, there is sufficient diversity among the various communities in the computing field that this may not be desirable at all. We do believe that the broad definitions provided above provide a framework that will allow badges to have general comparability among communities.

           Acronym = CHES 2025
Evaluation Process = Where possible, such as in software-based artifacts relying solely on open-source components, the artifact review process will aim to run the artifact and test harness, and see that it produces outputs that would be required to assess the artifact against results in the paper. For artifacts that depend on commercial tools or specialized physical hardware, the goal of the artifact review process will be to confirm that the artifacts are functional (should the submitters wish to be evaluated for functionality) and could plausibly be used by someone with access to the appropriate tools to reproduce the results.

           Acronym = CGO 2025
Evaluation Process = Authors of accepted CGO 2025 papers are invited to formally submit their supporting materials to the Artifact Evaluation (AE) process. The Artifact Evaluation Committee attempts to reproduce (at least the main) experiments and assesses if submitted artifacts support the claims made in the paper. 

           Acronym = CGO 2025
Evaluation Process = Each submitted artifact is evaluated by at least two members of the artifact evaluation committee.
During the process authors and evaluators are allowed to anonymously communicate with each other to overcome technical difficulties.
Ideally, we hope to see all submitted artifacts to successfully pass artifact evaluation.
The evaluators are asked to evaluate the artifact based on the following criteria, that are defined by ACM.

           Acronym = DSN 2025
Evaluation Process = The artifacts will be evaluated by a dedicated Artifacts Evaluation (AE) committee through a single-blind review process, where authors should be available to respond quickly during the artifact evaluation.
The artifact evaluation process is restricted to accepted papers in the research track of DSN (including PER and Tool papers). The evaluation will begin after the review process is complete and acceptance decisions have been made by the research track PC. The research PC chairs will make the submitted paper available to the Artifact Evaluation committee. The information about the artifact evaluation is NOT shared with the research PC in any form.

           Acronym = DSN 2025
Evaluation Process = Evaluation starts with a “kick-the-tires” period, during which evaluators ensure they can access their assigned artifacts and perform basic operations such as building and running a minimal working example. During the kick-the-tires period, the committee can communicate with the authors (anonymously through the submission platform) to give early feedback about the artifact, giving authors the option to address any significant blocking issues. After the kick-the-tires stage ends, communication can only address interpretation concerns for the produced results or minor syntactic issues in the submitted materials.

           Acronym = DSN 2025
Evaluation Process = We recommend authors to present and document artifacts in a way that the evaluation committee can use it and complete the evaluation successfully with minimal (and ideally no) interaction. To ensure that your instructions are complete, we suggest that you run through them on a fresh setup prior to submission, following exactly the instructions you have provided.
We expect that most evaluations can be done on any moderately-recent desktop or laptop computer. In other cases and to the extent possible, authors have to arrange their artifacts so as to run in community research testbeds or will provide remote access to their systems (e.g., via SSH) with proper anonymization. If the artifact is aimed at full reproducibility of results, but they take a long time to obtain (e.g., because of a large number of experiments, such as in fault injection), authors should provide a shortcut or sampling mechanism.

           Acronym = ICFP 2025
Evaluation Process = No Need for Anonymization
We use a single-blind review process. This means that while authors will not know the identity of reviewers, reviewers on the author hand will know who the authors of a paper are.

           Acronym = IEEE
Evaluation Process = IEEE Xplore document pages have badges on the top of the page to denote the availability of this supplemental information

           Acronym = NDSS 2025
Evaluation Process = Evaluation starts with a kick-the-tires period during which evaluators ensure they can access their assigned artifacts and perform basic operations such as building and running a minimal working example. Artifact evaluations include feedback about the artifact, giving authors the option to address any significant blocking issues for AE work using this feedback. Communication after the kick-the-tires stage end can address interpretation concerns for the produced results or minor syntactic issues in the submitted materials.

           Acronym = POPL 2025
Evaluation Process = In the first “kick the tires” phase reviewers download and install the artifact (if relevant) and exercise the basic functionality of the artifact to ensure that it works. We recommend authors include explicit instructions for this step. Failing the first phase—so that reviewers are unable to download and install the artifact—will prevent the artifact from being accepted.

           Acronym = POPL 2025
Evaluation Process = Milestone 1: Kick the Tires
Research software is delicate and needs careful setup. In order to ease this process, in the first phase of artifact evaluation, you will be expected to at least install the artifact and run a minimum set of commands (usually provided in the README by the authors) to sanity check that the artifact is correctly installed.

Here is a suggested process with some questions you can try to answer.

After reading the paper:

Q1: What is the central contribution of the paper?
Q2: What claims do the authors make of the artifact, and how does it connect to Q1 above?
Q3: Can you locate the specific, significant experimental claims made in the paper (such as figures, tables, etc.)?
Q4: What do you expect as a reasonable range of deviations for the experimental results?
After installing the artifact:

Q5: Are you able to install and test the artifact as indicated by the authors in their “kick the tires” instructions?
Q6: Are there any significant modifications you needed to make to the artifact while answering Q5?
Q7: For each claim highlighted in Q3 above, do you know how to reproduce the result, using the artifact?
Q8: Is there anything else that the authors or other reviewers should be aware of

           Acronym = POPL 2025
Evaluation Process = In the second “evaluation” phase reviewers systematically evaluate all claims in the paper via procedures included in the artifact to ensure consistency, completeness, documentation, and reusability. We recommend authors list all claims in the paper and indicate how to evaluate each claim using the artifact.

           Acronym = POPL 2025
Evaluation Process = Milestone 2: Evaluating Functionality 
After the kick-the-tires phase, you will perform an actual review of the artifact.

During this phase, here is a suggested list of questions to answer:

Q9: Does the artifact provide evidence for all the claims you noted in Q3? This corresponds to the completeness criterion of your evaluation.
Q10: Do the results of running / examining the artifact meet your expectations after having read the paper? This corresponds to the criterion of consistency between the paper and the artifact
Q11: Is the artifact well-documented, to the extent that answering questions Q5–Q10 is straightforward? Are the steps to reproduce results clear? (Note: by well-documented, for this stage, we are considering generally only the README and instructions – we don’t mean that the code itself needs to be documented. That would matter only for reusability if the intention would be to modify the code in some way.)

           Acronym = POPL 2025
Evaluation Process = Milestone 3: Evaluating Reusability
Finally, you will evaluate artifacts for reusability in new settings. To evaluate reusability, the following three initial questions are suggested for all artifacts:

Q12: If you were doing follow-up research in this area, do you think you would be able to reuse the paper as a baseline in your own work?
Q13: Is the code released via an open source license (e.g., released with an OSI approved license)? Is it made publicly available on a platform such as GitHub, GitLab, or BitBucket?
Q14: Does the artifact have clear installation instructions?
New this year, to help you evaluate proof artifacts, the remaining questions are different for traditional (software) artifacts and for proof artifacts. For traditional software artifacts:

Q15a: Are you able to modify the benchmarks / artifact to run simple additional experiments, similar to, but beyond those discussed in the paper?
For proof artifacts, instead of Q15a, we suggest answering:

Q15b: Does the proof artifact contain definitions and proofs that can be used in other projects? (Examples of such artifacts include Coq or Isabelle proof libraries and Coq plugins.)
Q16: Does the artifact clearly state all environment dependencies, including supported versions of the proof assistant and required third-party packages?
Q17: Are all proofs claimed as reusable complete? (no “admit” in Coq or “sorry” in Lean/Isabelle)

           Acronym = USENIX Sec 2025
Evaluation Process = Each artifact submission will be reviewed by at least two AEC members. The review is single-blind and strictly confidential. All AEC members are instructed to treat the artifact confidentially during and after completing evaluation and to not retain any part of the artifact after evaluation. Artifacts can include models, data files, proprietary binaries, exploits under embargo, etc. Even if authors cannot make their artifacts publicly accessible (e.g., proprietary files), they could still apply for Artifacts Functional and Results Reproduced. Since we anticipate small glitches with installation and use, reviewers may communicate with authors during artifact evaluation to help resolve glitches while preserving reviewer anonymity. Please make sure that at least one of the authors is reachable to answer questions in a timely manner.

           Acronym = USENIX Sec 2025
Evaluation Process = Most of the duration of this phase involves a single-blinded discussion period between the authors and AEC members. During this, the AEC members will work with the authors to help them improve the quality of their artifacts and make them amenable to the badges that they apply for. The AE timeline was set up to ensure approximately four weeks of time are allotted for this important discussion period. Throughout this period, the authors are expected to be available and improve their artifacts as per the feedback from the AEC. To kickstart this evaluation, authors can initially make the artifacts for this phase available on software development repositories (such as GitHub or GitLab) or Internet-accessible hardware owned/leased by the authors, containers/VMs, or any other reasonable format that enables evaluation.

           Acronym = USENIX Sec 2025
Evaluation Process = Initial “kick the tires” phase
Once you have been assigned artifacts, the initial “kick the tires” period starts. The goal of this period is to quickly determine whether you have everything you need for a full review: the artifact itself, any necessary hardware or other dependencies, and a plan on how you will evaluate the artifact. If that is not the case, you must discuss with your fellow evaluators and let the authors know of any problems as soon as possible, so that they have enough time to fix issues.
Double-check which badges the authors requested in their artifact submission; you do not need to evaluate the artifact for badges that were not requested (if you believe an artifact already meets the requirements for a badge the authors did not request, ask the authors; they may have forgotten to request that badge).
Carefully read the artifact documentation. In particular, check the software and hardware dependencies to make sure you have all you need. You are allowed to use your own judgment when making decisions, for instance to evaluate reasons why some artifacts may not be able to reproduce everything their paper contains. Before starting the evaluation, consider the following points and ideally share the evaluation plan with the authors:
- Whether you have everything you need to do the evaluation, and if not, what is missing, including:
  - Access to the necessary hardware owned by you or by the authors
  - For artifacts requesting the “functional” badge, documentation and full source code as mentioned in the checklist, and whether the code compiles
  - For artifacts requesting the “reproduced” badge, additionally the scripts to run the experiments and generate figures as mentioned in the checklist
- A plan on how you will evaluate the artifact during the review period:
   - Time frames of when experiments will be run in case hardware is shared

           Acronym = USENIX Sec 2025
Evaluation Process = Reviewing artifacts
For each artifact you are assigned to, you will produce one review explaining which badges you believe should be awarded and why or why not. You will work with the authors to produce your review, as this is a cooperative process. Authors are a resource you can use, exclusively through HotCRP comments, if you have trouble with an artifact or if you need more details about specific portions of an artifact.
There is an example review at the end of this guide.
First, (re-)read the page on badges. The checklists are particularly important and useful: artifacts that meet these requirements should get the corresponding badges, while artifacts that do not should either justify why or not get the badges. If an artifact does not satisfy a checklist but the authors provide a good reason as to why they should get the badge anyway, use your judgment based on the definitions of the badges. Remember that the Artifacts Functional and Results Reproduced badges require not only running the code but also auditing it to ensure that (for Artifacts Functional) the code is documented and understandable, and (for Results Reproduced) the code actually does what the paper states it does and reproduces results to support all the main claims of the paper (which must be documented in the submitted artifact appendix). Merely reproducing similar output as the paper, such as performance metrics, is not enough, the artifact must actually do what it claims to do. You are not expected to understand every single line of code, but you should be confident that the artifact overall matches the paper’s description.
Most of your time should be spent auditing artifacts, not debugging them. If you run into issues such as missing dependencies, try to quickly work around them, such as by finding the right package containing the dependency for your operating system and letting the authors know they have to fix their instructions. However, it is the authors’ responsibility to make their artifacts work, not yours. You do not need to spend hours trying to debug and fix complex issues; if you encounter a non-trivial error, first ask your fellow evaluators if they encountered it too or if they know how to fix it, then ask the authors to fix it.
It is acceptable to deny badges if artifacts require unreasonable effort, especially if such effort could be avoided through automation. For instance, if reproducing a claim requires 50 points of data, and the artifact requires you to manually edit 5 config files then run 4 commands on 3 machines for each data point, you do not need to actually perform hundreds of manual steps; instead, ask the authors to automate this, or even write a script yourself if you have the time that you can then share with the authors.
Concerning the artifact appendix, please verify it follows the provided template and its constraints (mandatory sections in particular). Do ask the authors for updates during the review process if the appendix does not follow the template or if important information is missing.
Once you are finished evaluating an artifact, fill in the review form and submit it at your earliest convenience. Your review must explain in detail why the artifact should or should not get each of the badges that the authors requested. You can also include additional suggestions for the authors to improve their artifacts if you have any. Note that you can edit your review as many times as you like, since reviews only become visible to the authors when final decisions are announced.
Remember that the artifact evaluation process is cooperative, not adversarial. Give authors a chance to fix issues by discussing through HotCRP comments before deciding that their artifact should not get a badge. In other words, help the authors improve their artifacts and reach badge status in the allocated time, whenever possible. However, if authors are being unresponsive or unreasonable, feel free to post a comment stating a badge cannot be awarded unless the authors take the specified steps in time by the deadline.
HotCRP allows you to rate your fellow evaluators’ reviews. If you think a review is well done, don’t hesitate to add a positive vote! If you think a review could use improvement, you can leave a negative vote and a reviewer discussion comment explaining your thoughts.

           Acronym = OSDI 2025
Evaluation Process = Review Process
The review process is structured in two phases:
Kick-the-tires: During this phase, reviewers will check for any obvious problems that prevent the artifact from being fully reviewed. Such problems include invalid download links, broken virtual machine images, missing dependencies, or failures when applying the artifact to a "Hello world"-sized example. Authors can respond to issues and provide an updated version of their artifact during a kick-the-tires response period.
Full evaluation: After the kick-the-tires phase, reviewers will fully evaluate the artifact.
#+end_verbatim

This is super interesting but I'm not sure it's part of the coding of guidance…

* Documentation

#+begin_src sqlite :results raw :wrap verbatim :line yes
   select distinct Acronym, "Documentaion" from data where "Documentaion"
   <> "";
#+end_src

#+RESULTS:
#+begin_verbatim
     Acronym = ICFP 2025
Documentaion = Readme
In most cases, the step-by-step instructions in your README.md should be a list of commands to build and test the artifact on the examples described in the paper and to reproduce any graphs and benchmarking results. The instructions should call out particular features of the results, such as “this produces the graph in Fig. 5 that shows our algorithm runs in linear time”. Try to keep the instructions clear enough, so that reviewers can work through them in under 30 minutes. Consider providing a top-level Makefile so that the commands to be executed are just make targets that automatically build their prerequisites.

If the build process emits warning messages, perhaps when building libraries that are not under the author’s control, include a note in the instructions that this is the case. Without such a note, the reviewers may assume something is wrong with the artifact itself.

Separately from the step-by-step instructions, provide other details about what a reviewer should look at. For example, “our artifact extends existing system X and our extension is the code located in file Y”.

     Acronym = ICFP 2025
Documentaion = Documentation
The artifact should contain sufficient documentation for reviewers to perform the activities mentioned above.
For programs, it should be clear how to build the program and how to run it on the examples provided in the paper.
For benchmarks, it should be clear how to run the benchmark and how to interpret the resulting data.
For formal proofs, it should be clear:
how to check that the proofs are axiom-free;
which parts of the formal proof correspond to which theorem in the paper;
how the notation and definitions used for the formal proof correspond to those used in the paper.

     Acronym = ICFP 2025
Documentaion = Documentation
The artifact should be documented in a way that facilitates reuse. This means:
There should be installation instructions for all supported operating systems. Dependencies should be clearly documented.
For programs, it should be clear how to run the program on inputs other than those from the paper. For example, for a compiler, the concrete syntax of the input language should be documented. Any options to the program should be documented. The main parts of the implementation should be documented to a reasonable degree. It should be clear how to run the test suite (if any).
For benchmarks, it should be clear how to run the benchmark on inputs other than those from the paper and how to prepare such inputs.
For formal proofs, the main parts of the proof (key lemmas and definitions) should be documented, especially if the notation differs from that used in the paper.

     Acronym = PLDI 2025
Documentaion = Documentation
Your artifact should include a README in a common format such as Markdown, plain text, or PDF, which should consist of two parts:

a Getting Started Guide and
Step-by-Step Instructions for how you propose to evaluate your artifact (with appropriate connections to the relevant sections of your paper);
The Getting Started Guide should contain setup instructions (including, for example, a pointer to the VM player software, its version, passwords if needed, etc.) and basic testing of your artifact that you expect a reviewer to be able to complete in 30 minutes. Reviewers will follow all the steps in the guide during an initial kick-the-tires phase. The Getting Started Guide should be as simple as possible, and yet it should stress the key elements of your artifact. Anyone who has followed the Getting Started Guide should have no technical difficulties with the rest of your artifact.

The Step by Step Instructions explain how to reproduce any experiments or other activities that support the conclusions in your paper. Write this for readers who have a deep interest in your work and are studying it to improve it or compare against it. If your artifact runs for more than a few minutes, point this out and explain how to run it on smaller inputs.

Where appropriate, include descriptions of and links to files (included in the archive) that represent expected outputs (e.g., the log files expected to be generated by your tool on the given inputs); if there are warnings that are safe to be ignored, explain which ones they are.

The artifact’s documentation should include the following:

A list of claims from the paper supported by the artifact, and how/why.
A list of claims from the paper not supported by the artifact, and how/why.

     Acronym = POPL 2025
Documentaion = In our experience, the key to a successful artifact evaluation is a good README file! Reviewers (and future researchers) will appreciate long, detailed, and clearly organized instructions which describe every aspect of your artifact in detail – including, e.g., shell commands to run, files to open, how long these will take, and what output is expected. This is not only to help artifact evaluation go smoothly – it provides confidence that members of the community will be able to replicate your results and use your tool for their own work in the future.

     Acronym = POPL 2025
Documentaion = List of claims
The list of claims should list all claims made in the paper. For each claim, provide a reference to the claim in the paper, the portion of the artifact evaluating that claim, and the evaluation instructions for evaluating that claim. The artifact need not support every claim in the paper; when evaluating the completeness of an artifact, reviewers will weigh the centrality and importance of the supported claims. Listing each claim individually provides the reviewer with a checklist to follow during the second, evaluation phase of the process. Organize the list of claims by section and subsection of the paper. A claim might read,

Theorem 12 from Section 5.2 of the paper corresponds to the theorem “foo” in the Coq file “src/Blah.v” and is evaluated in Step 7 of the evaluation instructions.

Some artifacts may attempt to perform malicious operations by design. Boldly and explicitly flag this in the instructions so AEC members can take appropriate precautions before installing and running these artifacts.

Reviewers expect artifacts to be buggy, immature, and have obscure error messages. Explicitly listing all claims allows the author to delineate which bugs invalidate the paper’s results and which are simply a normal part of the software engineering process.

     Acronym = POPL 2025
Documentaion = Download, installation, and sanity-testing
The download, installation, and sanity-testing instructions should contain complete instructions for obtaining a copy of the artifact and ensuring that it works. List any software the reviewer will need (such as virtual machine host software) along with version numbers and platforms that are known to work. Then list all files the reviewer will need to download (such as the virtual machine image) before beginning. Downloads take time, and reviewers prefer to complete all downloads before beginning evaluation.

Note the guest OS used in the virtual machine, and any unusual modifications made to it. Explain its directory layout. It’s a good idea to put your artifact on the desktop of a graphical guest OS or in the home directory of a terminal-only guest OS.

Installation and sanity-testing instructions should list all steps necessary to set up the artifact and ensure that it works. This includes explaining how to invoke the build system; how to run the artifact on small test cases, benchmarks, or proofs; and the expected output. Your instructions should make clear which directory to run each command from, what output files it generates, and how to compare those output files to the paper. If your artifact generates plots, the sanity testing instructions should check that the plotting software works and the plots can be viewed.

Helper scripts that automate building the artifact, running it, and viewing the results can help reviewers out. Test those scripts carefully—what do they do if run twice?

Aim for the download, installation, and sanity-testing instructions to be completable in about a half hour. Remember that reviewers will not know what error messages mean or how to circumvent errors. The more foolproof the artifact, the easier evaluation will be for them and for you.

     Acronym = POPL 2025
Documentaion = Evaluation instructions
The evaluation instructions should describe how to run the complete artifact, end to end, and then evaluate each claim in the paper that the artifact supports. This section often takes the form of a series of commands that generate evaluation data, and then a claim-by-claim list of how to check that the evaluation data is similar to the claims in the paper.

For each command, note the output files it writes to, so the reviewer knows where to find the results. If possible, generate data in the same format and organization as in the paper: for a table, include a script that generates a similar table, and for a plot, generate a similar plot.

Indicate how similar you expect the artifact results to be. Program speed usually differs in a virtual machine, and this may lead to, for example, more timeouts. Indicate how many you expect. 

     Acronym = POPL 2025
Documentaion = Additional artifact description
The additional description should explain how the artifact is organized, which scripts and source files correspond to which experiments and components in the paper, and how reviewers can try their own inputs to the artifact. For a mechanical proof, this section can point the reviewer to key definitions and theorems.

Expect reviewers to examine this section if something goes wrong (an unexpected error, for example) or if they are satisfied with the artifact and want to explore it further.

Reviewers expect that new inputs can trigger bugs, flag warnings, or behave oddly. However, describing the artifact’s organization lends credence to claims of reusability. Reviewers may also want to examine components of the artifact that interest them.

Remember that the AEC is attempting to determine whether the artifact meets the expectations set by the paper. (The instructions to the committee are available at the “Reviewer Guidelines” tab above.) Package your artifact to help the committee easily evaluate this.

     Acronym = OSDI 2025
Documentaion = README instructions: Your artifact package must include an obvious "README" that describes your artifact and provides a road map for evaluation. The README must consist of two sections. A "Getting Started Instructions" section should help reviewers check the basic functionality of the artifact within a short time frame (e.g., within 30 minutes). Such instructions could, for example, be on how to build a system and apply it to a "Hello world"-sized example. The purpose of this section is to allow reviewers to detect obvious problems during the kick-the-tires phase (e.g., a broken virtual machine image). A "Detailed Instructions" section should provide suitable instructions and documentation to fully evaluate the artifact.
#+end_verbatim

| Acronym   | Codes                               |
|-----------+-------------------------------------|
| ICFP 2025 | script,QC,guide,time,infrastructure |
| PLDI 2025 |                                     |
