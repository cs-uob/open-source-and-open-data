#+title: Open Source and Open Data Research Group Meeting Notes
#+author: Joseph Hallett, François Dupressoir, Awais Rashid, Miquel Perello Nieto and Inah Omoronyia

* <2024-04-25 Thu> Inaugral Meeting

- Joseph is working group leader.

- What do we want to do?
  - Why do we care? What is /open/ and /good/ research and why is it desirable
    - Well because funders like it... as well as researchers
    - What are the requirements for it from:
      - funders?
      - conferences?
      - researchers?
  - Defaults as part of guidance
    - What's the benefit of doing various things?
    - What's the benefit of various licenses?

- Are we making research /open/ or /reproducible/ and are they really the same thing?

- What sort of things are there and whose doing the current guidance?
  - Datasets :: ESRC
  - Sourcecode :: Github I guess?
  - Artefacts :: ???
  - ? Study design :: OSF

- Could we provide a reference implementation for how to do all this stuff?
  - Ooh that sounds like a good idea!
  - ...and we should probably write a paper about how we came to it!

- There is a Git repo for everything on the school Github account
  - Is Github /really/ open source though?
  - We should really put all our notes in it.
    - See _this_.

- We should review whats the current /state of the art/
  - Practices
  - Processes
  - Training initiatives
  - ...isn't this basically what we were saying earlier about looking at whats in the current guidance?
  - What about things like Jupyter notebooks?
    - Code goes in PURE
    - Data goes in RSDF
    - Isn't it frustrating there is no single source for this?

- Lets put all the notes from this working group online!
  - Open Source our process for designing what open research should look like!
  - Eat our own dog food!
  - But should we really do that? Who cares?
    - You don't have to look at it but if we're gonna recommend openness we ought to show our working?
  - We could do an /auto ethnographic study/ of how to do OS data sharing initiatives
    - And publish it as a CSCW paper! :-D
    - And in the spirit of dog fooding we should pre register it.
      - These notes are the semi structured input for how we did it!

- But should we /really/ make everything accessible to everyone?
  - You don't have to push to Git...
  - Other working groups don't make everything accessible, just end product...?-
  - Could we do /Chatham house/ rules?
  - We don't want to have detailed minutes...
    - Agreed: largely pointless
  - Keeping everything may slow us down
    - We could use Github features to track tasks and issues
      - Or a big org-mode file.
        - Members of working group:
          - 2 Emacs users
          - 2 Vim users
          - 1 VS code user
  - Jo largely favours making everything open
    - But he may be in the /Qowat Milat/...
      
** DONE François to hand Git things over                          :François:
** DONE Awais to set up weekly meetings                              :Awais:
Fortnightly, til term picks up proper for next year.
** DONE Jo to stick notes in Git                                        :Jo:
** Next time!

- We should think about building a big table aligning what is needed for Open Source and Open Research and what does what
  - And mapping it
  - And Jo should probably do it
- We should work out how to pre-register this whole experiment on OSF

* <2024-05-24 Fri> Meeting #2: Scoping things out

- Meeting held online, because it's half term.
- Inah away travelling.

** DONE Go look at dataset requirements for open source              :Awais:

- EPSRC's guidance is a bit /wishy-washy/
- ERC have the best dataset guidance
  - [[https://www.ukri.org/publications/esrc-research-data-policy/][ESRC Research Data Policy]]
  - UK data archive and Bristol's guidance are derivatives of the ERC's guidance
- What are we doing here? What's our /goal/?
  - We should be producing templates
    - These are the typical things you need to do for open science
    - Here are specific things you need to if you're doing /X/.
    - DOI needed for citation
  - ?? I thought this was what we agreed ??
  
** DONE Go look at what licenses are recommended for research       :Miquel:
- [[https://tailor-uob.github.io/new_ways_of_publishing/open-research/index.html][Open Research guidelines]] have a flowchart that we should probably link to.
- [[https://choosealicense.com/][Tool for choosing an opensource license]]
- Awais asked if funders sometimes require a licence?
  - Suspect people pick the same licenses always because they're the ones that have been used in the past
    - Is it our job to change that though?
  - Nation Security Investment Act and EPSRC Trusted research guidelines have requirements about licenses
    - The RED team at Bristol are useful to talk to about this
- Are there different licenses for code and data?
  - Don't seem to be ones for both or they're very Creative Commons-y
  - Open Data Commons
  - Governmental licenses for data
    
** DONE Go look at OSF pre-registration requirements              :François:
- They're completely unintelligible
  
** DONE Go look at artefact requirements at conferences                 :Jo:
*** ACM Guidelines

[[https://www.acm.org/publications/policies/artifact-review-and-badging-current]]

- Repeatability :: Same team, same experimental setup
- Reproducibility :: Different team, same experimental setup
- Replicability :: Different team, different experimental setup

  
Three sets of badges:

- Artifacts evaluated :: someone has checked that there is an
  inventory of whats in the data and it is consistent with what is in
  the paper.  Ideally it ought to reproduce everything, and their
  ought to be a script to generate the results.  A shinier /resuable/
  badge is awarded if it's also documented properly and not minimum
  effort.
- Artifacts available :: You stuck it on GitHub and got a DOI.
- Results validated :: Awarded posthoc for reproduction or replication.

*** Review procedures

Seems to be completely ad-hoc but the telling quote is:

#+begin_quote
``We believe that it is still too early to establish more specific
guidelines for artifact and replicability review. Indeed, there is
sufficient diversity among the various communities in the computing
field that this may not be desirable at all.''
#+end_quote

*** How to survive an artifact evaluation with HotCRP

#+begin_src bibtex
  @Unpublished{demetrescu2015survive,
    author =       {Camil Demetrescu},
    title =        {How to survive an artifact evaluation with HotCRP},
    note =         {Google Docs note.},
    year =      2015,
    url = {https://docs.google.com/document/d/1_Fq4mq5VJs-sMnBs39rTCEDWktb_qhcSeL3d9Kr4cD0/edit#heading=h.pvs9x3i3frny}}
#+end_src

Guide for how to set up processes using HotCRP.  Not much more than
give me a URL and a hash, and provide some bare bones (2 lines of and
HTTP form) descriptions.
** LATE Go look at funders requirements                               :Inah:
** TODO Start standard paperwork for preregistration              :Jo:Awais:

#+begin_quote
*Jo*:
Wotcha!  We’re meant to be registering our study on how to register
studies on Friday and we’ve got an action on us to draft a method and
do some of the boiler plate stuff.  Do you know what exactly we’ll
need then I can do a quick draft tomorrow and you can edit it?

*Awais*:
I have been too over-run with other things to think about this in
detail. I would see this as auto-ethnography - where we are doing the
work and also collecting data through observations and notes. We could
present it in the style of ethnographic research but I would suggest
we collect data (we have meeting notes but can also collect
observations/reflections as we conduct particular tasks, e.g.,
registering the study on OSF is itself one of those tasks … yes gets
very meta ). I would say once we have the data, we do a thematic
analysis (could do full blown GT but probably not ideal timewise) and
thematic analysis works better here IMO as we are looking at this
through the lens of open data and open source research artefacts. So
we do already have a particular stance. If we do go for thematic
analysis, we can just use adapt an existing description of
that. Suggest we follow Braun and Clarke to keep things sensible. I’d
suggest we are clear which incarnation of their approach (latest
book?).

*Jo*:
That implies ive read their latest book ;-) but my infinite reading
list is always looking for more… alright will put something together
and agree full gt not needed.  Reflective log very much way to go
#+end_quote

At this point I went and read (reread?) their papers (/Using thematic
analysis in psychology/ (2006), /Reflecting on reflexive thematic
analysis/ (2019) and /One size fits all?/ (2021)).

I also read the [[https://www2.sigsoft.org/EmpiricalStandards/docs/standards?standard=MetaScience#][ACM's Empirical Standards for Metascience]].

*** Our method

We are going to be running an autoethnographic study looking at how a working group exploring how to do open science came up with some guidelines for how to do open science.  Our method for analysis will be following Braun and Clarke's rough method for reflexive thematic analysis, taking the approach set out in their 2006 paper detailing the method with adaptations to bring it in line with the current thinking in their 2021 paper and the current /reflexive/ method.

**** What counts as a theme?
Our process, struggles and realisations and mental models as we attempt to work out how to do open science and open research.  Our themes will capture our beliefs about how this should be done, and what we found hard while attempting the process.  As such, they will form stories about what to do and how to avoid problems.

**** What is the data set?
A collection of notes, logs, minutes and other ephemera collected as we went about this process.

**** Are we doing inductive or theoretical thematic analysis?
We are doing theoretical thematic analysis as we are all active researchers, some of whom who do usability research and all of whom have created studies before.  We are clearly not impartial and have an idea about what we're looking for. The entire study is grounded in our experience of doing science and our findings describe the struggles we faced, and in capturing that experience and documenting what we learned as scientists in the field. 

**** Semantic or latent coding?
We will be aiming to code semantically, coding what we see rather than what we interpret as being the cause for something being hard and the struggles.  Our themes, as we develop them, may start to tell the stories as to what we felt was the causes for our issues and our guidance, but at the initial coding level we will aim to capture what we saw and did.

**** Realist or constructivist thematic analysis?
Since this is autoethnographic research for other computer science researchers attempting to do open science, we are taking a realist approach, though we accept that the whole thing is grounded in our reality as computer science researchers.

**** What version of thematic analysis is this?
It's predominantly a reflexive approach.  We're not going to be calculating kappas (who would second code anyway... it could never be impartial).  We are looking to clarify the struggles and our thoughts on the process of creating open science guidelines.  We will end up creating and presenting a codebook, but the output is what we learned from that process.

**** ACM Empirical Standards for Meta-Science Attributes

***** Essential Attributes
- [X] Presents information useful for other researchers
- [X] Makes recommendations for future research
- [X] Presents clear, valid arguments supporting recommendations (on the basis of our thematic analysis).

***** Desirable Attributes
- [ ] Synthesizes related work from reference disciplines
- [X] Provides insight specfically for software engineering
- [ ] Results integrated back into prior theory or research
- [X] Develops helpful artefacts (guidance and checklists and templates)

***** Extraordinary Attributes
- [X] Includes an empirical study (this is what we're doing)
- [ ] Quantitative simulation illustrating methodological issues

** TODO Create initial template of research access plan              :Awais:
** TODO Check with REPHRAIN admin if theres a current template       :Awais:
** TODO Ask why people picked licenses for data before              :Miquel:
** Next time!
We are going to preregister the study we want to run and figure out how to actually do this.

- <2024-06-07 Fri> Meeting to be skipped, because it clashes with the cyber poster day.
* <2024-06-21 Fri> Meeting #3 Preregistration attempt 1
Inah pulled out; Awais sends apologies.

Attempted to preregister study.  See transcript.

[[file:../preregistration/01-preregistration.txt][Transcript of session.]]
[[file:../preregistration/01-preregistration.mov][Screen recording of session.]]

* <2024-07-05 Fri> Meeting #4 Preregistration attempt 2
Awais sends apologies.
